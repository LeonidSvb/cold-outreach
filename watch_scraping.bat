@echo off
title Scraping Progress Monitor
:loop
cls
py -c "import pandas as pd; import json; from pathlib import Path; from datetime import datetime; csv_file = Path('modules/scraping/homepage_email_scraper/results/aus_client_deep_search/incremental_results.csv'); checkpoint_file = Path('modules/scraping/homepage_email_scraper/results/aus_client_deep_search/checkpoint.json'); checkpoint = json.load(open(checkpoint_file, 'r')); df = pd.read_csv(csv_file); emails_found = len(df); unique_companies = df['name'].nunique(); processed_urls = checkpoint.get('total_processed', 0); stats = checkpoint.get('stats', {}); print('=' * 80); print('SCRAPING PROGRESS - LIVE STATUS'); print('=' * 80); print(f'Time: {datetime.now().strftime(\"%%H:%%M:%%S\")}'); print(); print('EMAILS FOUND'); print(f'  Total emails: {emails_found:,}'); print(f'  Companies: {unique_companies:,}'); print(); print('PROCESSING STATUS'); print(f'  URLs processed: {processed_urls:,} / 10,408'); print(f'  Progress: {(processed_urls/10408*100):.1f}%%'); print(); print('SUCCESS METRICS'); print(f'  Success: {stats.get(\"success\", 0):,} ({(stats.get(\"success\", 0)/processed_urls*100 if processed_urls > 0 else 0):.1f}%%)'); print(f'  Failed: {stats.get(\"failed\", 0):,}'); print(f'  Emails from homepage: {stats.get(\"emails_from_homepage\", 0):,}'); print(f'  Emails from deep search: {stats.get(\"emails_from_deep\", 0):,}'); print(f'  No emails found: {stats.get(\"no_emails\", 0):,}'); print(); print('AUDIENCE BREAKDOWN'); [print(f'  {audience}: {count:,} emails') for audience, count in df['audience'].value_counts().items()]; print('=' * 80); print(); print('Auto-refresh every 15 seconds... Press Ctrl+C to stop')"
timeout /t 15 /nobreak >nul 2>&1
goto loop
