#!/usr/bin/env python3
"""
TEXT-ONLY Website Intelligence Scraper
–£–ª—å—Ç—Ä–∞-–±—ã—Å—Ç—Ä–∞—è –≤–µ—Ä—Å–∏—è —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –¢–û–õ–¨–ö–û —Ç–µ–∫—Å—Ç–∞ –∏ —Å—Å—ã–ª–æ–∫ (–±–µ–∑ HTML)

–û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò:
- –¢–æ–ª—å–∫–æ —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç (–±–µ–∑ HTML —Ç–µ–≥–æ–≤)
- –¢–æ–ª—å–∫–æ —Å—Å—ã–ª–∫–∏ (–±–µ–∑ CSS/JS)
- –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏
"""

import sys
import os
import time
import json
import pandas as pd
from datetime import datetime
import threading
import queue
from concurrent.futures import ThreadPoolExecutor, as_completed
import urllib.request
import urllib.parse
import urllib.error
import ssl
import re
from html.parser import HTMLParser
from typing import List, Dict, Set, Optional, Tuple

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ .env
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from dotenv import load_dotenv
load_dotenv()

TEXT_ONLY_CONFIG = {
    "max_http_workers": 50,      # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–æ—Ç–æ–∫–∏ (–º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö)
    "max_ai_workers": 8,         # –ë–æ–ª—å—à–µ AI –ø–æ—Ç–æ–∫–æ–≤ (–º–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤)
    "batch_size_tokens": 20000,  # –ë–æ–ª—å—à–∏–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
    "max_pages_per_domain": 15,  # –ú–µ–Ω—å—à–µ —Å—Ç—Ä–∞–Ω–∏—Ü (—Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω—ã–µ)
    "http_timeout": 8,           # –ë—ã—Å—Ç—Ä—ã–π —Ç–∞–π–º–∞—É—Ç
    "max_text_length": 2000,     # –ú–∞–∫—Å–∏–º—É–º —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
    "retry_attempts": 1,         # –ú–µ–Ω—å—à–µ –ø–æ–≤—Ç–æ—Ä–æ–≤
    "save_raw_data": True
}

class TextOnlyParser(HTMLParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¢–û–õ–¨–ö–û —Ç–µ–∫—Å—Ç–∞ –∏ —Å—Å—ã–ª–æ–∫"""
    
    def __init__(self):
        super().__init__()
        self.text_content = []
        self.links = []
        self.current_tag = None
        self.base_domain = ""
        self.ignore_tags = {'script', 'style', 'nav', 'footer', 'header', 'aside', 'meta', 'link'}
        
    def handle_starttag(self, tag, attrs):
        self.current_tag = tag
        if tag == 'a':
            for attr in attrs:
                if attr[0] == 'href' and attr[1]:
                    href = attr[1].strip()
                    if href and not href.startswith(('#', 'javascript:', 'mailto:', 'tel:')):
                        self.links.append(href)
                        
    def handle_endtag(self, tag):
        self.current_tag = None
        
    def handle_data(self, data):
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–æ–ª–µ–∑–Ω—ã–π —Ç–µ–∫—Å—Ç
        if self.current_tag not in self.ignore_tags:
            text = data.strip()
            if len(text) > 10:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
                self.text_content.append(text)
    
    def get_clean_data(self):
        """–ü–æ–ª—É—á–∏—Ç—å –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç –∏ –æ—á–∏—â–∞–µ–º
        full_text = ' '.join(self.text_content)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        clean_text = re.sub(r'\s+', ' ', full_text)
        clean_text = re.sub(r'[^\w\s\.\,\!\?\-\(\)]', ' ', clean_text)
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã —Å—Å—ã–ª–æ–∫
        unique_links = list(set(self.links))
        
        return {
            "text": clean_text[:TEXT_ONLY_CONFIG["max_text_length"]],
            "text_length": len(clean_text),
            "links": unique_links[:20]  # –ú–∞–∫—Å–∏–º—É–º 20 —Å—Å—ã–ª–æ–∫
        }

class TextOnlyWebsiteScraper:
    """–£–ª—å—Ç—Ä–∞-–±—ã—Å—Ç—Ä—ã–π —Å–∫—Ä–µ–π–ø–µ—Ä —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–∞"""
    
    def __init__(self):
        self.session_id = f"text_only_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.start_time = time.time()
        self.phase1_results = {}
        self.phase2_results = {}
        self.progress_stats = {
            "phase1_completed": 0,
            "phase1_total": 0,
            "phase2_completed": 0,
            "phase2_total": 0,
            "errors": [],
            "total_text_extracted": 0,
            "total_links_found": 0
        }
        self.lock = threading.Lock()
        
    def log(self, message: str, level: str = "INFO"):
        """–ü–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        # –£–±–∏—Ä–∞–µ–º —ç–º–æ–¥–∑–∏ –¥–ª—è Windows –∫–æ–Ω—Å–æ–ª–∏
        clean_message = message.encode('ascii', errors='ignore').decode('ascii')
        print(f"[{timestamp}] [{level}] {clean_message}")
    
    def scrape_page_text_only(self, url: str) -> Dict:
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–∞ –∏ —Å—Å—ã–ª–æ–∫"""
        try:
            # HTTP –∑–∞–ø—Ä–æ—Å —Å –±—ã—Å—Ç—Ä—ã–º —Ç–∞–π–º–∞—É—Ç–æ–º
            ctx = ssl.create_default_context()
            ctx.check_hostname = False
            ctx.verify_mode = ssl.CERT_NONE
            
            req = urllib.request.Request(url, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            
            with urllib.request.urlopen(req, timeout=TEXT_ONLY_CONFIG["http_timeout"], context=ctx) as response:
                # –ß–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 50KB (–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è —Ç–µ–∫—Å—Ç–∞)
                content = response.read(50000).decode('utf-8', errors='ignore')
                
                # –ü–∞—Ä—Å–∏–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç –∏ —Å—Å—ã–ª–∫–∏
                parser = TextOnlyParser()
                parser.base_domain = urllib.parse.urlparse(url).netloc
                parser.feed(content)
                
                clean_data = parser.get_clean_data()
                
                return {
                    "url": url,
                    "success": True,
                    "text": clean_data["text"],
                    "text_length": clean_data["text_length"],
                    "links": clean_data["links"],
                    "links_count": len(clean_data["links"])
                }
                
        except Exception as e:
            return {
                "url": url,
                "success": False,
                "error": str(e),
                "text": "",
                "text_length": 0,
                "links": [],
                "links_count": 0
            }
    
    def scrape_domain_text_only(self, domain: str) -> Dict:
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –¥–æ–º–µ–Ω–∞"""
        start_time = time.time()
        
        result = {
            "domain": domain,
            "pages": [],
            "total_text_length": 0,
            "total_links": 0,
            "success": False,
            "scrape_time": 0,
            "error": None
        }
        
        try:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–æ–º–µ–Ω
            if not domain.startswith(('http://', 'https://')):
                domain_url = f"https://{domain}"
            else:
                domain_url = domain
            
            # –°–∫—Ä–µ–π–ø–∏–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            main_page = self.scrape_page_text_only(domain_url)
            pages_data = []
            
            if main_page["success"]:
                pages_data.append(main_page)
                
                # –ù–∞—Ö–æ–¥–∏–º –≤–∞–∂–Ω—ã–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏
                base_domain = urllib.parse.urlparse(domain_url).netloc
                important_pages = []
                
                for link in main_page["links"]:
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å—Å—ã–ª–∫—É
                    if link.startswith('/'):
                        full_link = urllib.parse.urljoin(domain_url, link)
                    elif link.startswith(('http://', 'https://')):
                        full_link = link
                    else:
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Ç–æ—Ç –∂–µ –¥–æ–º–µ–Ω
                    if urllib.parse.urlparse(full_link).netloc == base_domain:
                        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –≤–∞–∂–Ω—ã–º —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
                        link_lower = full_link.lower()
                        if any(keyword in link_lower for keyword in 
                              ['about', 'services', 'product', 'team', 'contact', 'case', 'work', 'portfolio']):
                            important_pages.append(full_link)
                
                # –°–∫—Ä–µ–π–ø–∏–º –≤–∞–∂–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ (–º–∞–∫—Å–∏–º—É–º 10)
                important_pages = important_pages[:TEXT_ONLY_CONFIG["max_pages_per_domain"]]
                
                if important_pages:
                    with ThreadPoolExecutor(max_workers=5) as executor:
                        future_to_url = {executor.submit(self.scrape_page_text_only, url): url 
                                       for url in important_pages}
                        
                        for future in as_completed(future_to_url, timeout=30):
                            try:
                                page_data = future.result()
                                if page_data["success"]:
                                    pages_data.append(page_data)
                            except:
                                pass  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—à–∏–±–∫–∏
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            total_text_length = sum(p["text_length"] for p in pages_data)
            total_links = sum(p["links_count"] for p in pages_data)
            
            result.update({
                "pages": pages_data,
                "total_text_length": total_text_length,
                "total_links": total_links,
                "success": len(pages_data) > 0,
                "pages_count": len(pages_data)
            })
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            with self.lock:
                self.progress_stats["total_text_extracted"] += total_text_length
                self.progress_stats["total_links_found"] += total_links
                
        except Exception as e:
            result["error"] = str(e)
        
        result["scrape_time"] = time.time() - start_time
        return result
    
    def phase1_mass_text_scraping(self, domains: List[str]):
        """–§–ê–ó–ê 1: –ú–∞—Å—Å–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞"""
        self.log(f"üìù –§–ê–ó–ê 1: –ú–∞—Å—Å–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å {len(domains)} –¥–æ–º–µ–Ω–æ–≤ ({TEXT_ONLY_CONFIG['max_http_workers']} –ø–æ—Ç–æ–∫–æ–≤)")
        
        with self.lock:
            self.progress_stats["phase1_total"] = len(domains)
            self.progress_stats["phase1_completed"] = 0
        
        def progress_monitor():
            while True:
                time.sleep(10)
                with self.lock:
                    completed = self.progress_stats["phase1_completed"]
                    total = self.progress_stats["phase1_total"]
                    if completed >= total:
                        break
                    
                    elapsed = time.time() - self.start_time
                    rate = completed / (elapsed / 60) if elapsed > 0 else 0
                    eta = (total - completed) / rate if rate > 0 else 0
                    
                    self.log(f"üìä –ü–†–û–ì–†–ï–°–°: {completed}/{total} ({(completed/total)*100:.1f}%) | "
                           f"–°–∫–æ—Ä–æ—Å—Ç—å: {rate:.1f} –¥–æ–º–µ–Ω–æ–≤/–º–∏–Ω | ETA: {eta:.1f}–º–∏–Ω | "
                           f"–¢–µ–∫—Å—Ç–∞: {self.progress_stats['total_text_extracted']} —Å–∏–º–≤ | "
                           f"–°—Å—ã–ª–æ–∫: {self.progress_stats['total_links_found']}")
        
        monitor_thread = threading.Thread(target=progress_monitor, daemon=True)
        monitor_thread.start()
        
        # –ú–∞—Å—Å–æ–≤—ã–π —Å–∫—Ä–µ–π–ø–∏–Ω–≥ —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–æ—Ç–æ–∫–æ–≤
        with ThreadPoolExecutor(max_workers=TEXT_ONLY_CONFIG["max_http_workers"]) as executor:
            future_to_domain = {executor.submit(self.scrape_domain_text_only, domain): domain 
                              for domain in domains}
            
            for future in as_completed(future_to_domain):
                domain = future_to_domain[future]
                try:
                    result = future.result()
                    self.phase1_results[domain] = result
                    
                    with self.lock:
                        self.progress_stats["phase1_completed"] += 1
                        if not result["success"]:
                            self.progress_stats["errors"].append(f"–¢–µ–∫—Å—Ç –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω: {domain}")
                            
                except Exception as e:
                    with self.lock:
                        self.progress_stats["phase1_completed"] += 1
                        self.progress_stats["errors"].append(f"–û—à–∏–±–∫–∞ {domain}: {str(e)}")
        
        successful_domains = sum(1 for r in self.phase1_results.values() if r["success"])
        total_text = sum(r["total_text_length"] for r in self.phase1_results.values())
        
        self.log(f"–§–ê–ó–ê 1 –ó–ê–í–ï–†–®–ï–ù–ê: {successful_domains}/{len(domains)} –¥–æ–º–µ–Ω–æ–≤, "
                f"{total_text} —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞ –∏–∑–≤–ª–µ—á–µ–Ω–æ")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
        if TEXT_ONLY_CONFIG["save_raw_data"]:
            raw_file = f"raw_text_data_{self.session_id}.json"
            with open(raw_file, 'w', encoding='utf-8') as f:
                json.dump(self.phase1_results, f, indent=2, ensure_ascii=False)
            self.log(f"–°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {raw_file}")
    
    def prepare_text_ai_batches(self) -> List[Dict]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–∞—Ç—á–∏ –¥–ª—è AI (—Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç)"""
        self.log("ü§ñ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ AI –±–∞—Ç—á–µ–π (—Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç)...")
        
        batches = []
        current_batch = []
        current_tokens = 0
        
        for domain, result in self.phase1_results.items():
            if not result["success"]:
                continue
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫—Ä–∞—Ç–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è AI
            domain_summary = {
                "domain": domain,
                "pages_summary": []
            }
            
            for page in result["pages"]:
                if page["success"]:
                    # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞ + URL
                    page_summary = {
                        "url": page["url"],
                        "text_preview": page["text"][:300],
                        "text_length": page["text_length"]
                    }
                    domain_summary["pages_summary"].append(page_summary)
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã (–º–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ = –±—ã—Å—Ç—Ä–µ–µ)
            estimated_tokens = sum(len(p["text_preview"]) for p in domain_summary["pages_summary"]) // 4 + 100
            
            if current_tokens + estimated_tokens > TEXT_ONLY_CONFIG["batch_size_tokens"] and current_batch:
                batches.append({
                    "domains_data": current_batch.copy(),
                    "estimated_tokens": current_tokens
                })
                current_batch = [domain_summary]
                current_tokens = estimated_tokens
            else:
                current_batch.append(domain_summary)
                current_tokens += estimated_tokens
        
        if current_batch:
            batches.append({
                "domains_data": current_batch.copy(),
                "estimated_tokens": current_tokens
            })
        
        self.log(f"üì¶ –°–æ–∑–¥–∞–Ω–æ {len(batches)} AI –±–∞—Ç—á–µ–π, —Å—Ä–µ–¥–Ω–µ–µ —Ä–∞–∑–º–µ—Ä: {len(current_batch)} –¥–æ–º–µ–Ω–æ–≤")
        return batches
    
    def process_text_ai_batch(self, batch: Dict) -> Dict:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –±–∞—Ç—á —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ —á–µ—Ä–µ–∑ AI"""
        batch_start = time.time()
        domains_data = batch["domains_data"]
        
        try:
            from openai import OpenAI
            client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            
            # –ö—Ä–∞—Ç–∫–∏–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            prompt = f"""Analyze these {len(domains_data)} company websites and identify the 3 most valuable pages for B2B outreach personalization.

Look for: About Us, Services, Case Studies, Team, Recent Work, Products.

Return JSON array:
[{{
  "domain": "example.com",
  "selected_pages": [
    {{"url": "page_url", "value_score": 0.9, "reason": "brief reason"}},
    ...
  ]
}}]

Data: {json.dumps(domains_data, ensure_ascii=False)}"""

            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=3000  # –ú–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ = –±—ã—Å—Ç—Ä–µ–µ
            )
            
            ai_result = response.choices[0].message.content
            
            try:
                parsed_results = json.loads(ai_result)
                batch_results = {}
                
                for domain_result in parsed_results:
                    domain = domain_result.get("domain")
                    if any(d["domain"] == domain for d in domains_data):
                        batch_results[domain] = {
                            "ai_analysis": domain_result,
                            "processing_time": time.time() - batch_start,
                            "success": True
                        }
                
                return {
                    "success": True,
                    "results": batch_results,
                    "processing_time": time.time() - batch_start,
                    "domains_count": len([d["domain"] for d in domains_data]),
                    "cost": response.usage.total_tokens * 0.000002
                }
                
            except json.JSONDecodeError as e:
                return {
                    "success": False,
                    "error": f"JSON parse error: {str(e)}",
                    "raw_response": ai_result[:300]
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "processing_time": time.time() - batch_start
            }
    
    def phase2_text_ai_processing(self):
        """–§–ê–ó–ê 2: AI –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        batches = self.prepare_text_ai_batches()
        
        if not batches:
            self.log("‚ùå –ù–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è AI –∞–Ω–∞–ª–∏–∑–∞")
            return
            
        self.log(f"ü§ñ –§–ê–ó–ê 2: AI –∞–Ω–∞–ª–∏–∑ {len(batches)} –±–∞—Ç—á–µ–π —Ç–µ–∫—Å—Ç–∞ ({TEXT_ONLY_CONFIG['max_ai_workers']} –ø–æ—Ç–æ–∫–æ–≤)")
        
        with self.lock:
            self.progress_stats["phase2_total"] = len(batches)
            self.progress_stats["phase2_completed"] = 0
        
        total_cost = 0
        total_processed = 0
        
        with ThreadPoolExecutor(max_workers=TEXT_ONLY_CONFIG["max_ai_workers"]) as executor:
            future_to_batch = {executor.submit(self.process_text_ai_batch, batch): i 
                             for i, batch in enumerate(batches)}
            
            for future in as_completed(future_to_batch):
                batch_idx = future_to_batch[future]
                try:
                    batch_result = future.result()
                    
                    if batch_result["success"]:
                        for domain, result in batch_result["results"].items():
                            self.phase2_results[domain] = result
                        
                        total_cost += batch_result.get("cost", 0)
                        total_processed += batch_result["domains_count"]
                        
                        self.log(f"‚úÖ AI –ë–∞—Ç—á {batch_idx+1}/{len(batches)}: {batch_result['domains_count']} –¥–æ–º–µ–Ω–æ–≤, "
                               f"${batch_result.get('cost', 0):.4f}")
                    else:
                        self.log(f"‚ùå AI –ë–∞—Ç—á {batch_idx+1} –æ—à–∏–±–∫–∞: {batch_result['error']}")
                        
                    with self.lock:
                        self.progress_stats["phase2_completed"] += 1
                        
                except Exception as e:
                    self.log(f"‚ùå AI –ë–∞—Ç—á {batch_idx+1} –∏—Å–∫–ª—é—á–µ–Ω–∏–µ: {str(e)}")
                    with self.lock:
                        self.progress_stats["phase2_completed"] += 1
        
        self.log(f"‚úÖ –§–ê–ó–ê 2 –ó–ê–í–ï–†–®–ï–ù–ê: {total_processed} –¥–æ–º–µ–Ω–æ–≤ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ, —Å—Ç–æ–∏–º–æ—Å—Ç—å: ${total_cost:.4f}")
    
    def save_text_results(self, output_file: str):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (—Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç –∏ –∞–Ω–∞–ª–∏–∑)"""
        self.log(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        
        final_data = []
        
        for domain, phase1_result in self.phase1_results.items():
            row_data = {
                "domain": domain,
                "text_extraction_success": phase1_result["success"],
                "total_text_length": phase1_result["total_text_length"],
                "total_links_found": phase1_result["total_links"],
                "pages_scraped": phase1_result.get("pages_count", 0),
                "scraping_time": phase1_result["scrape_time"],
                "scraping_error": phase1_result.get("error", "")
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º AI –∞–Ω–∞–ª–∏–∑ –µ—Å–ª–∏ –µ—Å—Ç—å
            if domain in self.phase2_results:
                ai_result = self.phase2_results[domain]
                row_data.update({
                    "ai_analysis_success": ai_result["success"],
                    "ai_selected_pages": json.dumps(ai_result.get("ai_analysis", {}), ensure_ascii=False),
                    "ai_processing_time": ai_result["processing_time"]
                })
            else:
                row_data.update({
                    "ai_analysis_success": False,
                    "ai_selected_pages": "",
                    "ai_processing_time": 0
                })
            
            final_data.append(row_data)
        
        df = pd.DataFrame(final_data)
        df.to_csv(output_file, index=False)
        
        self.log(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {len(final_data)} –∑–∞–ø–∏—Å–µ–π")
    
    def process_csv_text_only(self, input_file: str, limit: int = None):
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç)"""
        total_start = time.time()
        
        self.log(f"üìù TEXT-ONLY WEBSITE SCRAPER - –ó–ê–ü–£–°–ö")
        self.log(f"üìÅ –í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {input_file}")
        self.log(f"‚öôÔ∏è  –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: HTTP={TEXT_ONLY_CONFIG['max_http_workers']}, AI={TEXT_ONLY_CONFIG['max_ai_workers']}")
        self.log(f"üéØ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ: –¢–û–õ–¨–ö–û —Ç–µ–∫—Å—Ç –∏ —Å—Å—ã–ª–∫–∏ (–±–µ–∑ HTML)")
        
        # –ß–∏—Ç–∞–µ–º –¥–æ–º–µ–Ω—ã
        df = pd.read_csv(input_file)
        valid_domains = df[df['company_domain'].notna() & (df['company_domain'] != '')]
        
        if limit:
            domains = valid_domains['company_domain'].tolist()[:limit]
        else:
            domains = valid_domains['company_domain'].tolist()
        
        self.log(f"üìä –ö –æ–±—Ä–∞–±–æ—Ç–∫–µ: {len(domains)} –¥–æ–º–µ–Ω–æ–≤")
        
        # –§–ê–ó–ê 1: –ú–∞—Å—Å–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        self.phase1_mass_text_scraping(domains)
        
        # –§–ê–ó–ê 2: AI –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞
        self.phase2_text_ai_processing()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        output_file = f"text_only_results_{self.session_id}.csv"
        self.save_text_results(output_file)
        
        total_time = time.time() - total_start
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        successful_domains = len([d for d in self.phase1_results.values() if d["success"]])
        ai_processed = len(self.phase2_results)
        total_text = sum(r["total_text_length"] for r in self.phase1_results.values())
        total_links = sum(r["total_links"] for r in self.phase1_results.values())
        
        self.log(f"\n{'='*80}")
        self.log(f"üéâ TEXT-ONLY SCRAPER - –†–ï–ó–£–õ–¨–¢–ê–¢–´")
        self.log(f"{'='*80}")
        self.log(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.1f}—Å ({total_time/60:.1f}–º–∏–Ω)")
        self.log(f"üìä –¢–µ–∫—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω: {successful_domains}/{len(domains)} –¥–æ–º–µ–Ω–æ–≤")
        self.log(f"üìù –í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–∞: {total_text} —Å–∏–º–≤–æ–ª–æ–≤")
        self.log(f"üîó –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫: {total_links}")
        self.log(f"ü§ñ AI –∞–Ω–∞–ª–∏–∑: {ai_processed} –¥–æ–º–µ–Ω–æ–≤")
        self.log(f"üöÄ –°–∫–æ—Ä–æ—Å—Ç—å: {successful_domains/(total_time/60):.1f} –¥–æ–º–µ–Ω–æ–≤/–º–∏–Ω")
        self.log(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {output_file}")
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å HTML –≤–µ—Ä—Å–∏–µ–π
        html_time_estimate = successful_domains * 60  # 1 –º–∏–Ω –Ω–∞ –¥–æ–º–µ–Ω —Å HTML
        speedup = html_time_estimate / total_time if total_time > 0 else 1
        
        self.log(f"‚ö° –£–°–ö–û–†–ï–ù–ò–ï: –≤ {speedup:.1f}x –±—ã—Å—Ç—Ä–µ–µ HTML –≤–µ—Ä—Å–∏–∏!")
        self.log(f"üíæ –≠–ö–û–ù–û–ú–ò–Ø: {(1 - total_text/1000000):.1f}% –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö")
        
        return output_file

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    input_file = "../../leads/raw/lumid_canada_20250108.csv"
    
    if not os.path.exists(input_file):
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {input_file}")
        return
    
    scraper = TextOnlyWebsiteScraper()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ 10 –¥–æ–º–µ–Ω–∞—Ö –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏
    result_file = scraper.process_csv_text_only(input_file, limit=10)
    
    print(f"\n–î–õ–Ø –ü–û–õ–ù–û–ô –û–ë–†–ê–ë–û–¢–ö–ò –í–°–ï–• –î–û–ú–ï–ù–û–í:")
    print(f"  1. –£–±–µ—Ä–∏—Ç–µ limit=10")
    print(f"  2. –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è –¥–ª—è 745 –¥–æ–º–µ–Ω–æ–≤: 10-20 –º–∏–Ω—É—Ç")
    print(f"  3. –¢–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç + —Å—Å—ã–ª–∫–∏ (–±–µ–∑ HTML –º—É—Å–æ—Ä–∞)")

if __name__ == "__main__":
    main()