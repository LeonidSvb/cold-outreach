#!/usr/bin/env python3
"""
ULTRA PARALLEL Website Intelligence Scraper
–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±—ã—Å—Ç—Ä–∞—è 2-—Ñ–∞–∑–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ 2000+ —Å–∞–π—Ç–æ–≤

–ê–†–•–ò–¢–ï–ö–¢–£–†–ê:
–§–ê–ó–ê 1: –ú–∞—Å—Å–æ–≤—ã–π HTTP —Å–∫—Ä–µ–π–ø–∏–Ω–≥ (20-50 –ø–æ—Ç–æ–∫–æ–≤)
–§–ê–ó–ê 2: –ë–∞—Ç—á AI –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –±–∞—Ç—á–∏ –ø–æ —Ç–æ–∫–µ–Ω–∞–º)

–¶–ï–õ–¨: 2000 —Å–∞–π—Ç–æ–≤ –∑–∞ 15-30 –º–∏–Ω—É—Ç –≤–º–µ—Å—Ç–æ 5+ —á–∞—Å–æ–≤
"""

import sys
import os
import time
import json
import pandas as pd
from datetime import datetime
import threading
import queue
from concurrent.futures import ThreadPoolExecutor, as_completed
import urllib.request
import urllib.parse
import urllib.error
import ssl
import re
from html.parser import HTMLParser
from typing import List, Dict, Set, Optional, Tuple

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ .env
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from dotenv import load_dotenv
load_dotenv()

ULTRA_CONFIG = {
    "max_http_workers": 30,  # –ú–∞–∫—Å–∏–º—É–º HTTP –ø–æ—Ç–æ–∫–æ–≤
    "max_ai_workers": 5,     # –ú–∞–∫—Å–∏–º—É–º AI –ø–æ—Ç–æ–∫–æ–≤ (rate limit)
    "batch_size_tokens": 15000,  # –†–∞–∑–º–µ—Ä AI –±–∞—Ç—á–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º
    "max_pages_per_domain": 30,  # –õ–∏–º–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ –¥–æ–º–µ–Ω
    "http_timeout": 10,      # HTTP —Ç–∞–π–º–∞—É—Ç
    "retry_attempts": 2,     # –ü–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏
    "save_raw_data": True,   # –°–æ—Ö—Ä–∞–Ω—è—Ç—å —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
    "ai_model": "gpt-3.5-turbo"
}

class HTMLLinkExtractor(HTMLParser):
    """–ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫"""
    
    def __init__(self):
        super().__init__()
        self.links = []
        self.base_domain = ""
        
    def handle_starttag(self, tag, attrs):
        if tag == 'a':
            for attr in attrs:
                if attr[0] == 'href' and attr[1]:
                    href = attr[1].strip()
                    if href and not href.startswith('#') and not href.startswith('javascript:'):
                        self.links.append(href)

class UltraParallelScraper:
    """–£–ª—å—Ç—Ä–∞-–±—ã—Å—Ç—Ä—ã–π —Å–∫—Ä–µ–π–ø–µ—Ä —Å 2-—Ñ–∞–∑–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π"""
    
    def __init__(self):
        self.session_id = f"ultra_parallel_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.start_time = time.time()
        self.phase1_results = {}  # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã HTTP —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞
        self.phase2_results = {}  # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã AI –∞–Ω–∞–ª–∏–∑–∞
        self.progress_stats = {
            "phase1_completed": 0,
            "phase1_total": 0,
            "phase2_completed": 0,
            "phase2_total": 0,
            "errors": []
        }
        self.lock = threading.Lock()
        
    def log(self, message: str, level: str = "INFO"):
        """–ü–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] [{level}] {message}")
        
    def get_progress_info(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ"""
        with self.lock:
            elapsed = time.time() - self.start_time
            p1_pct = (self.progress_stats["phase1_completed"] / max(self.progress_stats["phase1_total"], 1)) * 100
            p2_pct = (self.progress_stats["phase2_completed"] / max(self.progress_stats["phase2_total"], 1)) * 100
            
            return (f"üöÄ –ü–†–û–ì–†–ï–°–°: Phase1 {self.progress_stats['phase1_completed']}/{self.progress_stats['phase1_total']} "
                   f"({p1_pct:.1f}%) | Phase2 {self.progress_stats['phase2_completed']}/{self.progress_stats['phase2_total']} "
                   f"({p2_pct:.1f}%) | –í—Ä–µ–º—è: {elapsed:.0f}—Å | –û—à–∏–±–∫–∏: {len(self.progress_stats['errors'])}")

    def scrape_domain_fast(self, domain: str) -> Dict:
        """–ë—ã—Å—Ç—Ä—ã–π HTTP —Å–∫—Ä–µ–π–ø–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞"""
        result = {
            "domain": domain,
            "pages": [],
            "total_pages": 0,
            "scrape_time": 0,
            "success": False,
            "error": None
        }
        
        start_time = time.time()
        
        try:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–æ–º–µ–Ω
            if not domain.startswith(('http://', 'https://')):
                domain_url = f"https://{domain}"
            else:
                domain_url = domain
                
            # –°–æ–∑–¥–∞–µ–º HTTP context —Å —Ç–∞–π–º–∞—É—Ç–æ–º
            ctx = ssl.create_default_context()
            ctx.check_hostname = False
            ctx.verify_mode = ssl.CERT_NONE
            
            # –°–∫—Ä–µ–π–ø–∏–º –æ—Å–Ω–æ–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            discovered_pages = set([domain_url])
            pages_data = []
            
            def scrape_page(url: str) -> Optional[Dict]:
                try:
                    req = urllib.request.Request(url, headers={
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                    })
                    
                    with urllib.request.urlopen(req, timeout=ULTRA_CONFIG["http_timeout"], context=ctx) as response:
                        content = response.read().decode('utf-8', errors='ignore')
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏
                        parser = HTMLLinkExtractor()
                        parser.feed(content)
                        
                        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Å—ã–ª–∫–∏
                        internal_links = []
                        base_domain = urllib.parse.urlparse(url).netloc
                        
                        for link in parser.links:
                            if link.startswith('/'):
                                full_link = urllib.parse.urljoin(url, link)
                                if urllib.parse.urlparse(full_link).netloc == base_domain:
                                    internal_links.append(full_link)
                            elif link.startswith(('http://', 'https://')):
                                if urllib.parse.urlparse(link).netloc == base_domain:
                                    internal_links.append(link)
                        
                        return {
                            "url": url,
                            "content": content[:5000],  # –ü–µ—Ä–≤—ã–µ 5KB
                            "title": self.extract_title(content),
                            "links": internal_links[:20],  # –ú–∞–∫—Å–∏–º—É–º 20 —Å—Å—ã–ª–æ–∫
                            "content_length": len(content)
                        }
                        
                except Exception as e:
                    return {"url": url, "error": str(e)}
            
            # –°–∫—Ä–µ–π–ø–∏–º –æ—Å–Ω–æ–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            main_page = scrape_page(domain_url)
            if main_page and "error" not in main_page:
                pages_data.append(main_page)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                for link in main_page.get("links", [])[:ULTRA_CONFIG["max_pages_per_domain"]]:
                    if link not in discovered_pages:
                        discovered_pages.add(link)
                        
                # –°–∫—Ä–µ–π–ø–∏–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ)
                additional_pages = list(discovered_pages - {domain_url})[:10]  # –ú–∞–∫—Å–∏–º—É–º 10 –¥–æ–ø —Å—Ç—Ä–∞–Ω–∏—Ü
                
                with ThreadPoolExecutor(max_workers=5) as executor:
                    future_to_url = {executor.submit(scrape_page, url): url for url in additional_pages}
                    
                    for future in as_completed(future_to_url, timeout=30):
                        try:
                            page_data = future.result()
                            if page_data and "error" not in page_data:
                                pages_data.append(page_data)
                        except Exception as e:
                            pass  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—à–∏–±–∫–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            
            result["pages"] = pages_data
            result["total_pages"] = len(pages_data)
            result["success"] = len(pages_data) > 0
            
        except Exception as e:
            result["error"] = str(e)
            
        result["scrape_time"] = time.time() - start_time
        return result
    
    def extract_title(self, content: str) -> str:
        """–ò–∑–≤–ª–µ—á—å title –∏–∑ HTML"""
        try:
            match = re.search(r'<title[^>]*>(.*?)</title>', content, re.IGNORECASE | re.DOTALL)
            if match:
                return match.group(1).strip()[:100]
        except:
            pass
        return "No title"
    
    def phase1_mass_http_scraping(self, domains: List[str]):
        """–§–ê–ó–ê 1: –ú–∞—Å—Å–æ–≤—ã–π HTTP —Å–∫—Ä–µ–π–ø–∏–Ω–≥"""
        self.log(f"üöÄ –§–ê–ó–ê 1: –ú–∞—Å—Å–æ–≤—ã–π HTTP —Å–∫—Ä–µ–π–ø–∏–Ω–≥ {len(domains)} –¥–æ–º–µ–Ω–æ–≤ —Å {ULTRA_CONFIG['max_http_workers']} –ø–æ—Ç–æ–∫–∞–º–∏")
        
        with self.lock:
            self.progress_stats["phase1_total"] = len(domains)
            self.progress_stats["phase1_completed"] = 0
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å-–º–æ–Ω–∏—Ç–æ—Ä –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        def progress_monitor():
            while True:
                self.log(self.get_progress_info())
                time.sleep(15)  # –ö–∞–∂–¥—ã–µ 15 —Å–µ–∫—É–Ω–¥
                with self.lock:
                    if self.progress_stats["phase1_completed"] >= self.progress_stats["phase1_total"]:
                        break
        
        monitor_thread = threading.Thread(target=progress_monitor, daemon=True)
        monitor_thread.start()
        
        # –ú–∞—Å—Å–æ–≤—ã–π —Å–∫—Ä–µ–π–ø–∏–Ω–≥
        with ThreadPoolExecutor(max_workers=ULTRA_CONFIG["max_http_workers"]) as executor:
            future_to_domain = {executor.submit(self.scrape_domain_fast, domain): domain for domain in domains}
            
            for future in as_completed(future_to_domain):
                domain = future_to_domain[future]
                try:
                    result = future.result()
                    self.phase1_results[domain] = result
                    
                    with self.lock:
                        self.progress_stats["phase1_completed"] += 1
                        if not result["success"]:
                            self.progress_stats["errors"].append(f"HTTP error for {domain}: {result.get('error', 'Unknown')}")
                            
                except Exception as e:
                    with self.lock:
                        self.progress_stats["phase1_completed"] += 1
                        self.progress_stats["errors"].append(f"Exception for {domain}: {str(e)}")
        
        successful_scrapes = sum(1 for r in self.phase1_results.values() if r["success"])
        total_pages = sum(r["total_pages"] for r in self.phase1_results.values())
        
        self.log(f"‚úÖ –§–ê–ó–ê 1 –ó–ê–í–ï–†–®–ï–ù–ê: {successful_scrapes}/{len(domains)} –¥–æ–º–µ–Ω–æ–≤, {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
        if ULTRA_CONFIG["save_raw_data"]:
            raw_file = f"raw_scraping_data_{self.session_id}.json"
            with open(raw_file, 'w', encoding='utf-8') as f:
                json.dump(self.phase1_results, f, indent=2, ensure_ascii=False)
            self.log(f"üíæ –°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {raw_file}")
    
    def prepare_ai_batches(self) -> List[Dict]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –±–∞—Ç—á–∏ –¥–ª—è AI –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        self.log("üì¶ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ AI –±–∞—Ç—á–µ–π –ø–æ —Ç–æ–∫–µ–Ω–∞–º...")
        
        batches = []
        current_batch = []
        current_tokens = 0
        
        for domain, result in self.phase1_results.items():
            if not result["success"]:
                continue
                
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã –¥–ª—è –¥–æ–º–µ–Ω–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ)
            domain_content = ""
            for page in result["pages"]:
                domain_content += page.get("content", "") + page.get("title", "")
            
            # –ì—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ (1 —Ç–æ–∫–µ–Ω ‚âà 4 —Å–∏–º–≤–æ–ª–∞)
            estimated_tokens = len(domain_content) // 4 + 200  # +200 –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
            
            if current_tokens + estimated_tokens > ULTRA_CONFIG["batch_size_tokens"] and current_batch:
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π –±–∞—Ç—á
                batches.append({
                    "domains": current_batch.copy(),
                    "estimated_tokens": current_tokens
                })
                current_batch = [domain]
                current_tokens = estimated_tokens
            else:
                current_batch.append(domain)
                current_tokens += estimated_tokens
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –±–∞—Ç—á
        if current_batch:
            batches.append({
                "domains": current_batch.copy(),
                "estimated_tokens": current_tokens
            })
        
        self.log(f"üì¶ –°–æ–∑–¥–∞–Ω–æ {len(batches)} AI –±–∞—Ç—á–µ–π, —Å—Ä–µ–¥–Ω–µ–µ —Ä–∞–∑–º–µ—Ä: {len(current_batch)} –¥–æ–º–µ–Ω–æ–≤")
        return batches
    
    def process_ai_batch(self, batch: Dict) -> Dict:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–∏–Ω AI –±–∞—Ç—á"""
        batch_start = time.time()
        domains = batch["domains"]
        
        try:
            import openai
            openai.api_key = os.getenv("OPENAI_API_KEY")
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            batch_data = []
            for domain in domains:
                result = self.phase1_results[domain]
                if result["success"]:
                    pages_info = []
                    for page in result["pages"]:
                        pages_info.append({
                            "url": page["url"],
                            "title": page.get("title", ""),
                            "content_preview": page.get("content", "")[:500]
                        })
                    batch_data.append({
                        "domain": domain,
                        "pages": pages_info
                    })
            
            # AI –ø—Ä–æ–º–ø—Ç –¥–ª—è –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∏
            prompt = f"""Analyze these {len(batch_data)} company websites and prioritize their most valuable pages for B2B outreach personalization.

For each domain, identify the 3 most valuable pages from: About, Services, Case Studies, Team, Recent News, Products.

Return JSON array with this structure:
[{{
  "domain": "example.com",
  "selected_pages": [
    {{"url": "page_url", "value_score": 0.9, "reason": "why valuable"}},
    ...
  ]
}}]

Website data: {json.dumps(batch_data, ensure_ascii=False)}"""

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
            response = openai.ChatCompletion.create(
                model=ULTRA_CONFIG["ai_model"],
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=4000
            )
            
            ai_result = response.choices[0].message.content
            
            # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
            try:
                parsed_results = json.loads(ai_result)
                batch_results = {}
                
                for domain_result in parsed_results:
                    domain = domain_result.get("domain")
                    if domain in domains:
                        batch_results[domain] = {
                            "ai_analysis": domain_result,
                            "processing_time": time.time() - batch_start,
                            "success": True
                        }
                
                return {
                    "success": True,
                    "results": batch_results,
                    "processing_time": time.time() - batch_start,
                    "domains_count": len(domains),
                    "cost": response.usage.total_tokens * 0.000002  # GPT-3.5 pricing
                }
                
            except json.JSONDecodeError as e:
                return {
                    "success": False,
                    "error": f"JSON parse error: {str(e)}",
                    "raw_response": ai_result[:500]
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "domains_count": len(domains),
                "processing_time": time.time() - batch_start
            }
    
    def phase2_batch_ai_processing(self):
        """–§–ê–ó–ê 2: –ë–∞—Ç—á AI –æ–±—Ä–∞–±–æ—Ç–∫–∞"""
        batches = self.prepare_ai_batches()
        
        if not batches:
            self.log("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è AI –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return
            
        self.log(f"ü§ñ –§–ê–ó–ê 2: –ë–∞—Ç—á AI –æ–±—Ä–∞–±–æ—Ç–∫–∞ {len(batches)} –±–∞—Ç—á–µ–π —Å {ULTRA_CONFIG['max_ai_workers']} –ø–æ—Ç–æ–∫–∞–º–∏")
        
        with self.lock:
            self.progress_stats["phase2_total"] = len(batches)
            self.progress_stats["phase2_completed"] = 0
        
        total_cost = 0
        total_processed = 0
        
        with ThreadPoolExecutor(max_workers=ULTRA_CONFIG["max_ai_workers"]) as executor:
            future_to_batch = {executor.submit(self.process_ai_batch, batch): i for i, batch in enumerate(batches)}
            
            for future in as_completed(future_to_batch):
                batch_idx = future_to_batch[future]
                try:
                    batch_result = future.result()
                    
                    if batch_result["success"]:
                        # –ú–µ—Ä–≥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                        for domain, result in batch_result["results"].items():
                            self.phase2_results[domain] = result
                        
                        total_cost += batch_result.get("cost", 0)
                        total_processed += batch_result["domains_count"]
                        
                        self.log(f"‚úÖ –ë–∞—Ç—á {batch_idx+1}/{len(batches)}: {batch_result['domains_count']} –¥–æ–º–µ–Ω–æ–≤, "
                               f"${batch_result.get('cost', 0):.4f}, {batch_result['processing_time']:.1f}—Å")
                    else:
                        self.log(f"‚ùå –ë–∞—Ç—á {batch_idx+1} error: {batch_result['error']}")
                        
                    with self.lock:
                        self.progress_stats["phase2_completed"] += 1
                        
                except Exception as e:
                    self.log(f"‚ùå Exception batch {batch_idx+1}: {str(e)}")
                    with self.lock:
                        self.progress_stats["phase2_completed"] += 1
        
        self.log(f"‚úÖ –§–ê–ó–ê 2 –ó–ê–í–ï–†–®–ï–ù–ê: {total_processed} –¥–æ–º–µ–Ω–æ–≤, –æ–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: ${total_cost:.4f}")
    
    def save_final_results(self, output_file: str):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ CSV"""
        self.log(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ {output_file}")
        
        final_data = []
        
        for domain, phase1_result in self.phase1_results.items():
            row_data = {
                "domain": domain,
                "scraping_success": phase1_result["success"],
                "total_pages_found": phase1_result["total_pages"],
                "scraping_time": phase1_result["scrape_time"],
                "scraping_error": phase1_result.get("error", "")
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º AI —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –µ—Å–ª–∏ –µ—Å—Ç—å
            if domain in self.phase2_results:
                ai_result = self.phase2_results[domain]
                row_data.update({
                    "ai_success": ai_result["success"],
                    "ai_analysis": json.dumps(ai_result.get("ai_analysis", {}), ensure_ascii=False),
                    "ai_processing_time": ai_result["processing_time"]
                })
            else:
                row_data.update({
                    "ai_success": False,
                    "ai_analysis": "",
                    "ai_processing_time": 0
                })
            
            final_data.append(row_data)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV
        df = pd.DataFrame(final_data)
        df.to_csv(output_file, index=False)
        
        self.log(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {len(final_data)} –∑–∞–ø–∏—Å–µ–π")
    
    def process_csv_ultra_fast(self, input_file: str, limit: int = None):
        """–£–ª—å—Ç—Ä–∞-–±—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ CSV —Ñ–∞–π–ª–∞"""
        total_start = time.time()
        
        self.log(f"üöÄ ULTRA PARALLEL SCRAPER - –ó–ê–ü–£–°–ö")
        self.log(f"üìÅ –í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {input_file}")
        self.log(f"‚öôÔ∏è  –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: HTTP –ø–æ—Ç–æ–∫–∏={ULTRA_CONFIG['max_http_workers']}, AI –ø–æ—Ç–æ–∫–∏={ULTRA_CONFIG['max_ai_workers']}")
        
        # –ß–∏—Ç–∞–µ–º –¥–æ–º–µ–Ω—ã
        df = pd.read_csv(input_file)
        valid_domains = df[df['company_domain'].notna() & (df['company_domain'] != '')]
        
        if limit:
            domains = valid_domains['company_domain'].tolist()[:limit]
        else:
            domains = valid_domains['company_domain'].tolist()
        
        self.log(f"üìä –ö –æ–±—Ä–∞–±–æ—Ç–∫–µ: {len(domains)} –¥–æ–º–µ–Ω–æ–≤")
        
        # –§–ê–ó–ê 1: –ú–∞—Å—Å–æ–≤—ã–π HTTP —Å–∫—Ä–µ–π–ø–∏–Ω–≥
        self.phase1_mass_http_scraping(domains)
        
        # –§–ê–ó–ê 2: –ë–∞—Ç—á AI –æ–±—Ä–∞–±–æ—Ç–∫–∞
        self.phase2_batch_ai_processing()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        output_file = f"ultra_parallel_results_{self.session_id}.csv"
        self.save_final_results(output_file)
        
        total_time = time.time() - total_start
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        successful_domains = len([d for d in self.phase1_results.values() if d["success"]])
        ai_processed = len(self.phase2_results)
        
        self.log(f"\n{'='*80}")
        self.log(f"üéâ ULTRA PARALLEL SCRAPER - –†–ï–ó–£–õ–¨–¢–ê–¢–´")
        self.log(f"{'='*80}")
        self.log(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.1f}—Å ({total_time/60:.1f}–º–∏–Ω)")
        self.log(f"üìä HTTP —Å–∫—Ä–µ–π–ø–∏–Ω–≥: {successful_domains}/{len(domains)} –¥–æ–º–µ–Ω–æ–≤")
        self.log(f"ü§ñ AI –∞–Ω–∞–ª–∏–∑: {ai_processed} –¥–æ–º–µ–Ω–æ–≤")
        self.log(f"üöÄ –°–∫–æ—Ä–æ—Å—Ç—å: {successful_domains/(total_time/60):.1f} –¥–æ–º–µ–Ω–æ–≤/–º–∏–Ω")
        self.log(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {output_file}")
        self.log(f"‚ùå –û—à–∏–±–∫–∏: {len(self.progress_stats['errors'])}")
        
        if self.progress_stats['errors']:
            self.log("üîç –ü–µ—Ä–≤—ã–µ 5 –æ—à–∏–±–æ–∫:")
            for error in self.progress_stats['errors'][:5]:
                self.log(f"  ‚Ä¢ {error}")
        
        return output_file

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    input_file = "../../leads/raw/lumid_canada_20250108.csv"
    
    if not os.path.exists(input_file):
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {input_file}")
        return
    
    scraper = UltraParallelScraper()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ 100 –¥–æ–º–µ–Ω–∞—Ö —Å–Ω–∞—á–∞–ª–∞
    result_file = scraper.process_csv_ultra_fast(input_file, limit=100)
    
    print(f"\nüéØ –î–õ–Ø –ü–û–õ–ù–û–ô –û–ë–†–ê–ë–û–¢–ö–ò 2000+ –î–û–ú–ï–ù–û–í:")
    print(f"  1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –±–µ–∑ limit parameter")
    print(f"  2. –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è: 20-40 –º–∏–Ω—É—Ç")
    print(f"  3. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ ULTRA_CONFIG –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏")

if __name__ == "__main__":
    main()